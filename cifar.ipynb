{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathname = \"cifar-10-batches-py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labelnames(dict):\n",
    "    return [item.decode(\"utf8\") for item in dict[b\"label_names\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y,t):\n",
    "    return np.sum((y-t)**2)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_diff(f,x):\n",
    "    h = 1e-4\n",
    "    return (f(x+h)-f(x-h))/(2*h) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y,t):\n",
    "    delta = 1e-7 # log(0)を防ぐため\n",
    "    if y.ndim == 1:\n",
    "        y = y.reshape(1,y.size)\n",
    "        t = t.reshape(1,t.size)\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t*np.log(y+delta))/batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f,x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros(x.shape)\n",
    "    for i in range(len(x)):\n",
    "        tmp = x[i]\n",
    "        x[i] = x[i] + h\n",
    "        fxhp = f(x)\n",
    "        x[i] = tmp\n",
    "        x[i] = x[i] - h\n",
    "        fxhm = f(x)\n",
    "        grad[i] = (fxhp - fxhm)/(2*h)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelname = get_labelnames(unpickle(pathname+\"/batches.meta\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_arr = []\n",
    "label_arr = []\n",
    "DATA_BATCH_NUM = 5\n",
    "for i in range(DATA_BATCH_NUM):\n",
    "    data_batch = unpickle(pathname+\"/data_batch_{}\".format(i+1))\n",
    "    data_arr.extend(data_batch[b'data'])\n",
    "    label_arr += data_batch[b'labels']\n",
    "data_arr = np.array(data_arr)\n",
    "label_arr = np.array(label_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    c = np.max(x)\n",
    "    return np.exp(x-c)/np.sum(np.exp(x-c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self,W,b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "    \n",
    "    def forward(self,x):\n",
    "        self.x = x\n",
    "        out = np.dot(x,self.W) + self.b\n",
    "        return out\n",
    "    def backward(self,dout):\n",
    "        dx = np.dot(dout,self.W.T)\n",
    "        self.dW = np.dot(self.x.T,dout)\n",
    "        self.db = np.sum(dout,axis=0)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "    def forward(self,x,t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y,self.t)\n",
    "        return self.loss\n",
    "    def backward(self,dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "    def forward(self,x):\n",
    "        out = 1/(1+np.exp(-x))\n",
    "        self.out = out\n",
    "        return out\n",
    "    def backward(self,dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNeuralNet():\n",
    "    def __init__(self,input_size=3072,hidden_size=20,output_size=10,weight_init_std=0.01):\n",
    "        self.params = {}\n",
    "        self.params[\"W1\"] = np.random.randn(input_size,hidden_size) * weight_init_std\n",
    "        self.params[\"B1\"] = np.zeros(hidden_size)\n",
    "        self.params[\"W2\"] = np.random.randn(hidden_size,output_size) * weight_init_std\n",
    "        self.params[\"B2\"] = np.zeros(output_size)\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers[\"Affine1\"] =  Affine(self.params['W1'], self.params['B1'])\n",
    "        self.layers[\"Relu1\"] = Relu()\n",
    "        self.layers[\"Affine2\"] =  Affine(self.params['W2'], self.params['B2'])\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "    def predict(self,x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def loss(self,x,t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y,t)\n",
    "\n",
    "    def gradient(self,x,t):\n",
    "        self.loss(x,t)\n",
    "        dout = self.lastLayer.backward(1) # \n",
    "        layers_tmp = list(self.layers.values())\n",
    "        layers_tmp.reverse()\n",
    "        for layer in layers_tmp:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        grads = {}\n",
    "        grads['W1'], grads['B1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W2'], grads['B2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "        return grads\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = TwoLayerNeuralNet(input_size=784, hidden_size=50, output_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_one_hot_arr(label_arr,label_num):\n",
    "    t_batch = np.array([])\n",
    "    batch_size = len(label_arr)\n",
    "    for label in label_arr:\n",
    "        t = np.zeros(label_num)\n",
    "        t[label] = 1\n",
    "        t_batch = np.append(t_batch,t)\n",
    "    return t_batch.reshape(batch_size,label_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'train_size' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-0129f9a22fb6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mloss_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miters_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mbatch_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mx_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_mask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mt_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_mask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_size' is not defined"
     ]
    }
   ],
   "source": [
    "iters_num = 10000\n",
    "train_size = data_arr.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "loss_arr = []\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    grad = nn.gradient(x_batch,t_batch)\n",
    "    for key in (\"W1\",\"B1\",\"W2\",\"B2\"):\n",
    "        nn.params[key] -= learning_rate * grad[key]\n",
    "    loss = nn.loss(x_batch,t_batch)\n",
    "    loss_arr.append(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "accurate_num = 0\n",
    "data_size = label_arr.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
     },
     "metadata": {},
     "execution_count": 101
    }
   ],
   "source": [
    "t_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])"
     },
     "metadata": {},
     "execution_count": 105
    }
   ],
   "source": [
    "nn.predict(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
    }
   ],
   "source": [
    "for i in range(60,100):\n",
    "    \n",
    "    # print([int(item) for item in softmax(nn.predict(x_train[i])) == np.max(softmax(nn.predict(x_train[i])))])\n",
    "    print(t_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.09784\n"
    }
   ],
   "source": [
    "# for i in range(data_size):\n",
    "#     if np.where(softmax(nn.predict(data_arr[i])) == np.max(softmax(nn.predict(data_arr[i])))) == np.where(softmax(nn.predict(data_arr[i])):\n",
    "#         accurate_num += 1\n",
    "# print(accurate_num/data_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "4892"
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "accurate_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "50000"
     },
     "metadata": {},
     "execution_count": 117
    }
   ],
   "source": [
    "data_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}